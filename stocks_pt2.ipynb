{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68cf5cd",
   "metadata": {
    "papermill": {
     "duration": 0.015331,
     "end_time": "2022-05-16T08:54:20.669976",
     "exception": false,
     "start_time": "2022-05-16T08:54:20.654645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**STOCKS ROBOT**\n",
    "\n",
    "* The purpose of this project is to maximize the profit from the stocks market during 2005-2021.\n",
    "* I only chose 3 popular tech stocks (GOOGL, AMZN, AAPL) for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e8adaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:54:20.704478Z",
     "iopub.status.busy": "2022-05-16T08:54:20.702969Z",
     "iopub.status.idle": "2022-05-16T08:55:39.823713Z",
     "shell.execute_reply": "2022-05-16T08:55:39.823016Z",
     "shell.execute_reply.started": "2022-05-14T01:35:44.358656Z"
    },
    "papermill": {
     "duration": 79.138347,
     "end_time": "2022-05-16T08:55:39.823867",
     "exception": false,
     "start_time": "2022-05-16T08:54:20.685520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\r\n",
      "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: lxml>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from yfinance) (4.6.3)\r\n",
      "Collecting multitasking>=0.0.7\r\n",
      "  Downloading multitasking-0.0.10.tar.gz (8.2 kB)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from yfinance) (1.3.3)\r\n",
      "Collecting requests>=2.26\r\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 63 kB 300 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.7/site-packages (from yfinance) (1.19.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->yfinance) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->yfinance) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.26->yfinance) (1.26.6)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.26->yfinance) (2.10)\r\n",
      "Collecting charset-normalizer~=2.0.0\r\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.26->yfinance) (2021.10.8)\r\n",
      "Building wheels for collected packages: multitasking\r\n",
      "  Building wheel for multitasking (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for multitasking: filename=multitasking-0.0.10-py3-none-any.whl size=8500 sha256=83d58011a9adc9f8bd9e681f630ba10c294ee0372cea7895ec8be0139146d524\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/ba/79/c0260c6f1a03f420ec7673eff9981778f293b9107974679e36\r\n",
      "Successfully built multitasking\r\n",
      "Installing collected packages: charset-normalizer, requests, multitasking, yfinance\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.25.1\r\n",
      "    Uninstalling requests-2.25.1:\r\n",
      "      Successfully uninstalled requests-2.25.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\r\n",
      "beatrix-jupyterlab 3.1.1 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "gcsfs 2021.8.1 requires fsspec==2021.08.1, but you have fsspec 2021.10.0 which is incompatible.\r\n",
      "apache-beam 2.32.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\r\n",
      "apache-beam 2.32.0 requires pyarrow<5.0.0,>=0.15.1, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "apache-beam 2.32.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\r\n",
      "Successfully installed charset-normalizer-2.0.12 multitasking-0.0.10 requests-2.27.1 yfinance-0.1.70\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Collecting stable-baselines3\r\n",
      "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 177 kB 288 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.9.1)\r\n",
      "Requirement already satisfied: gym==0.21 in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (0.21.0)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (2.0.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (3.4.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.3.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stable-baselines3) (1.19.5)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /opt/conda/lib/python3.7/site-packages (from gym==0.21->stable-baselines3) (4.8.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3) (3.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21->stable-baselines3) (3.10.0.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (0.10.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (8.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (1.3.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines3) (2.8.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->stable-baselines3) (1.16.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines3) (2021.1)\r\n",
      "Installing collected packages: stable-baselines3\r\n",
      "Successfully installed stable-baselines3-1.5.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.10.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.62.3)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (5.4.1)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.25)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\r\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.7.4)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (6.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (21.0)\r\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.7.1)\r\n",
      "Requirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.9.0)\r\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (4.8.1)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.1.5)\r\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (5.2.2)\r\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.2.0)\r\n",
      "Requirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (0.4.0)\r\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.4.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.6.0)\r\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\r\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\r\n",
      "Requirement already satisfied: colorama>=0.3.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (2.0.1)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Selecting previously unselected package libta-lib0.\r\n",
      "(Reading database ... 111661 files and directories currently installed.)\r\n",
      "Preparing to unpack libta.deb ...\r\n",
      "Unpacking libta-lib0 (0.4.0-oneiric1) ...\r\n",
      "Selecting previously unselected package ta-lib0-dev.\r\n",
      "Preparing to unpack ta.deb ...\r\n",
      "Unpacking ta-lib0-dev (0.4.0-oneiric1) ...\r\n",
      "Setting up libta-lib0 (0.4.0-oneiric1) ...\r\n",
      "Setting up ta-lib0-dev (0.4.0-oneiric1) ...\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\r\n",
      "Collecting ta-lib\r\n",
      "  Downloading TA-Lib-0.4.24.tar.gz (269 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 269 kB 249 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from ta-lib) (1.19.5)\r\n",
      "Building wheels for collected packages: ta-lib\r\n",
      "  Building wheel for ta-lib (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for ta-lib: filename=TA_Lib-0.4.24-cp37-cp37m-linux_x86_64.whl size=1464564 sha256=c6d9813924d0f72a2be158dc286a78ed9b3ad842b1d763cb16b6b1f68737feed\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/b1/a5/aca31e8cdd0137d7a83bf72237056b8705663dd1f9b5eac11e\r\n",
      "Successfully built ta-lib\r\n",
      "Installing collected packages: ta-lib\r\n",
      "Successfully installed ta-lib-0.4.24\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# install libraries\n",
    "!pip install yfinance\n",
    "!pip install stable-baselines3\n",
    "!pip install optuna\n",
    "url = \"https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files\"\n",
    "!wget $url/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
    "!wget $url/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
    "!dpkg -i libta.deb ta.deb\n",
    "!pip install ta-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c2844",
   "metadata": {
    "papermill": {
     "duration": 0.036682,
     "end_time": "2022-05-16T08:55:39.895667",
     "exception": false,
     "start_time": "2022-05-16T08:55:39.858985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* I also downloaded indices that might correlate to these tech stocks such as NASDAQ index. They may help with the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1ecd84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:39.971416Z",
     "iopub.status.busy": "2022-05-16T08:55:39.970542Z",
     "iopub.status.idle": "2022-05-16T08:55:44.439296Z",
     "shell.execute_reply": "2022-05-16T08:55:44.438804Z",
     "shell.execute_reply.started": "2022-05-14T01:36:57.572405Z"
    },
    "papermill": {
     "duration": 4.509511,
     "end_time": "2022-05-16T08:55:44.439449",
     "exception": false,
     "start_time": "2022-05-16T08:55:39.929938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  6 of 6 completed\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "import talib\n",
    "import yfinance as yf \n",
    "\n",
    "stocks = ['GOOGL'\n",
    "          , 'AMZN'\n",
    "          , 'AAPL'\n",
    "         ]\n",
    "          \n",
    "index = [\n",
    "          '^GSPC',\n",
    "          '^IXIC',\n",
    "          '^TNX'\n",
    "         ]\n",
    "yf_interval = \"1d\"\n",
    "\n",
    "df_o = yf.download(\n",
    "        tickers = stocks+index,            \n",
    "        interval = yf_interval,  \n",
    "        start=\"2005-01-01\"\n",
    "        , end=\"2021-12-31\"\n",
    "        , group_by = 'ticker',     \n",
    "        auto_adjust = True,      \n",
    "        prepost = True,          \n",
    "        threads = True,          \n",
    "        proxy = None)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd6d341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:44.517617Z",
     "iopub.status.busy": "2022-05-16T08:55:44.516689Z",
     "iopub.status.idle": "2022-05-16T08:55:44.522284Z",
     "shell.execute_reply": "2022-05-16T08:55:44.521865Z",
     "shell.execute_reply.started": "2022-05-14T01:36:59.305267Z"
    },
    "papermill": {
     "duration": 0.046897,
     "end_time": "2022-05-16T08:55:44.522426",
     "exception": false,
     "start_time": "2022-05-16T08:55:44.475529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "df = df_o.copy()\n",
    "idx_name = {'^IXIC':'NASDAQ', '^TNX':'BOND', '^GSPC':'SP500'}\n",
    "df.columns = [(i[0] if i[0] not in idx_name else idx_name[i[0]])+\"_\"+i[1] for i in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3014a492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:44.600123Z",
     "iopub.status.busy": "2022-05-16T08:55:44.599248Z",
     "iopub.status.idle": "2022-05-16T08:55:44.610987Z",
     "shell.execute_reply": "2022-05-16T08:55:44.611421Z",
     "shell.execute_reply.started": "2022-05-14T01:36:59.479698Z"
    },
    "papermill": {
     "duration": 0.052823,
     "end_time": "2022-05-16T08:55:44.611548",
     "exception": false,
     "start_time": "2022-05-16T08:55:44.558725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4279, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping column with wrong data\n",
    "df = df.drop(['BOND_Volume'],axis=1)\n",
    "\n",
    "# checking duplicates\n",
    "df = df.reset_index()\n",
    "df = df.drop_duplicates(subset=['Date'])\n",
    "df = df.set_index('Date')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9da7db",
   "metadata": {
    "papermill": {
     "duration": 0.036567,
     "end_time": "2022-05-16T08:55:44.684610",
     "exception": false,
     "start_time": "2022-05-16T08:55:44.648043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Here I added some common technical indicators that will help with the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a34fd1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:44.766448Z",
     "iopub.status.busy": "2022-05-16T08:55:44.765514Z",
     "iopub.status.idle": "2022-05-16T08:55:51.581937Z",
     "shell.execute_reply": "2022-05-16T08:55:51.580922Z",
     "shell.execute_reply.started": "2022-05-14T01:37:04.291443Z"
    },
    "papermill": {
     "duration": 6.861784,
     "end_time": "2022-05-16T08:55:51.582099",
     "exception": false,
     "start_time": "2022-05-16T08:55:44.720315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding extra features\n",
    "\n",
    "# rsi\n",
    "from talib import RSI\n",
    "\n",
    "for i in stocks:\n",
    "    df[i+'_rsi'] = RSI(df[i+'_Close'], timeperiod=14)\n",
    "\n",
    "# cci\n",
    "def CCI(df_main, i, ndays): \n",
    "    df = df_main.copy()\n",
    "    df[i+'_TP'] = (df[i+'_High'] + df[i+'_Low'] + df[i+'_Close']) / 3 \n",
    "    df[i+'_sma'] = df[i+'_TP'].rolling(ndays).mean()\n",
    "    df[i+'_mad'] = df[i+'_TP'].rolling(ndays).apply(lambda x: pd.Series(x).mad())\n",
    "    df[i+'_cci'] = (df[i+'_TP'] - df[i+'_sma']) / (0.015 * df[i+'_mad']) \n",
    "    return df[i+'_cci']\n",
    "\n",
    "for i in stocks:\n",
    "    df[i+'_cci'] = CCI(df, i, 20)\n",
    "\n",
    "# macd\n",
    "for i in stocks:\n",
    "    short_ema =  df[i+'_Close'].ewm(span=12, adjust=False).mean()\n",
    "    long_ema = df[i+'_Close'].ewm(span=26, adjust=False).mean()\n",
    "#     signal = df[i+'_Close'].ewm(span=9, adjust=False).mean()\n",
    "    df[i+'_macd'] =  short_ema - long_ema\n",
    "#     df[i+'_short_ema'] = short_ema\n",
    "\n",
    "# removing rows upto the needed history range of extra features\n",
    "df = df.iloc[26:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b16cd",
   "metadata": {
    "papermill": {
     "duration": 0.036023,
     "end_time": "2022-05-16T08:55:51.656791",
     "exception": false,
     "start_time": "2022-05-16T08:55:51.620768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* I used the data from 2005 to 2019 to train the model\n",
    "* year 2020 was used for validation, and 2021 for testing.\n",
    "* 60 days of price history was used as features for the deep learning model in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e87b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:51.732487Z",
     "iopub.status.busy": "2022-05-16T08:55:51.731834Z",
     "iopub.status.idle": "2022-05-16T08:55:51.734507Z",
     "shell.execute_reply": "2022-05-16T08:55:51.734066Z",
     "shell.execute_reply.started": "2022-05-14T01:37:11.594974Z"
    },
    "papermill": {
     "duration": 0.042036,
     "end_time": "2022-05-16T08:55:51.734631",
     "exception": false,
     "start_time": "2022-05-16T08:55:51.692595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################### MODELING  ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fb866a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:51.811503Z",
     "iopub.status.busy": "2022-05-16T08:55:51.810500Z",
     "iopub.status.idle": "2022-05-16T08:55:51.812550Z",
     "shell.execute_reply": "2022-05-16T08:55:51.813005Z",
     "shell.execute_reply.started": "2022-05-14T01:37:12.757125Z"
    },
    "papermill": {
     "duration": 0.042337,
     "end_time": "2022-05-16T08:55:51.813128",
     "exception": false,
     "start_time": "2022-05-16T08:55:51.770791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############## REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948ad8c",
   "metadata": {
    "papermill": {
     "duration": 0.052892,
     "end_time": "2022-05-16T08:55:51.901890",
     "exception": false,
     "start_time": "2022-05-16T08:55:51.848998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Next, I tried reinforcement learning model using PPO algorithm\n",
    "* The setting of the model is as below\n",
    "\n",
    "environment - stocks market with 3 tech stocks and no transaction fee.\n",
    "agent - the trading robot\n",
    "state - current cash, latest stocks price, technical indicator, current shares in the port.\n",
    "action - buy, sell upto the maximum share per trade\n",
    "reward - gain in price difference\n",
    "\n",
    "* Note that I had to preprocess the data again, since the technical indicators would be used also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8609c9a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:52.052125Z",
     "iopub.status.busy": "2022-05-16T08:55:52.051206Z",
     "iopub.status.idle": "2022-05-16T08:55:52.869170Z",
     "shell.execute_reply": "2022-05-16T08:55:52.869641Z",
     "shell.execute_reply.started": "2022-05-14T01:37:47.161755Z"
    },
    "papermill": {
     "duration": 0.903659,
     "end_time": "2022-05-16T08:55:52.869812",
     "exception": false,
     "start_time": "2022-05-16T08:55:51.966153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjusting data preprocessing\n",
    "# train test split\n",
    "df_train = df.loc[:'2019']\n",
    "df_val = df.loc['2020']\n",
    "df_test = df.loc['2021']\n",
    "\n",
    "# adding prior sequences for val and test\n",
    "length = 60\n",
    "df_val = pd.concat([df_train.iloc[-length:,:],df_val])\n",
    "df_test = pd.concat([df_val.iloc[-length:,:],df_test])\n",
    "\n",
    "# feature selection\n",
    "select_feat = ['Close',\n",
    "               'rsi',\n",
    "               'cci',\n",
    "               'macd'\n",
    "              ]\n",
    "\n",
    "feat_ls = [stocks+'_'+feat for stocks in stocks for feat in select_feat]\n",
    "\n",
    "out_col_ls = [stocks+'_Close' for stocks in stocks]\n",
    "\n",
    "df_train = df_train[feat_ls]\n",
    "df_val = df_val[feat_ls]\n",
    "df_test = df_test[feat_ls]\n",
    "\n",
    "def xy_split(df):\n",
    "    return df[feat_ls], df[out_col_ls]\n",
    "\n",
    "X_train, y_train = xy_split(df_train)\n",
    "X_val, y_val = xy_split(df_val)\n",
    "X_test, y_test = xy_split(df_test)\n",
    "\n",
    "# scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc_X = MinMaxScaler()\n",
    "sc_X.fit(X_train)\n",
    "X_train = sc_X.transform(X_train)\n",
    "X_val = sc_X.transform(X_val)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "sc_y = MinMaxScaler()\n",
    "sc_y.fit(y_train)\n",
    "y_train = sc_y.transform(y_train)\n",
    "y_val = sc_y.transform(y_val)\n",
    "y_test = sc_y.transform(y_test)\n",
    "\n",
    "# remove 0\n",
    "selected_idx = [i for i in range(len(X_train)) if X_train[i,0]>0 and X_train[i,1]>0 and X_train[i,2]>0]\n",
    "X_train = X_train[selected_idx]\n",
    "y_train = y_train[selected_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bae7c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:52.970160Z",
     "iopub.status.busy": "2022-05-16T08:55:52.969454Z",
     "iopub.status.idle": "2022-05-16T08:55:53.300076Z",
     "shell.execute_reply": "2022-05-16T08:55:53.299594Z",
     "shell.execute_reply.started": "2022-05-14T01:39:37.518152Z"
    },
    "papermill": {
     "duration": 0.393135,
     "end_time": "2022-05-16T08:55:53.300216",
     "exception": false,
     "start_time": "2022-05-16T08:55:52.907081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env train\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# shares normalization factor\n",
    "# max shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "MAX_ACCOUNT_BALANCE = 100e6\n",
    "MAX_SHARE = 1e6\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "TRANSACTION_FEE_PERCENT = 0\n",
    "REWARD_SCALING = 1e-4\n",
    "# price history\n",
    "FEATURES = len(select_feat)\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df_sc):\n",
    "\n",
    "        self.day = 1\n",
    "        self.df_sc = df_sc\n",
    "        self.df = sc_X.inverse_transform(df_sc)\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = [Current Balance]+[4 features for 3 stocks]+[owned shares for 3 stocks] \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (1+FEATURES*STOCK_DIM+STOCK_DIM,))\n",
    "        self.terminal = False     \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        \n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self._seed()\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        # update balance\n",
    "        # sell the amount suggested by action, but not more than the amount in the port\n",
    "        self.state[0] += \\\n",
    "        self.state[index*FEATURES+1]*min(abs(action),self.state[index+STOCK_DIM*FEATURES+1]) * \\\n",
    "         (1- TRANSACTION_FEE_PERCENT)\n",
    "        # update amount in the port\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] -= min(abs(action), self.state[index+STOCK_DIM*FEATURES+1])\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index*FEATURES+1]\n",
    "        # update cash. buy the amount suggested by action, limited by available cash\n",
    "        self.state[0] -= self.state[index*FEATURES+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)        \n",
    "        # update stocks balance\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] += min(available_amount, action)\n",
    "    \n",
    "    def step(self, actions):\n",
    "\n",
    "        self.terminal = self.day == self.df.shape[0]-1\n",
    "\n",
    "        if self.terminal:\n",
    "            return self.state_sc, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # normalize \n",
    "            actions = actions * HMAX_NORMALIZE   \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]           \n",
    "            \n",
    "            for index in sell_index:\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "\n",
    "            #load next state\n",
    "            self.state = [self.state[0]] +\\\n",
    "                          self.df[self.day-1].tolist() +\\\n",
    "                          self.state[FEATURES*STOCK_DIM+1:]\n",
    "        \n",
    "            self.state_sc = [self.state[0]/MAX_ACCOUNT_BALANCE] +\\\n",
    "                              self.df_sc[self.day-1].tolist() +\\\n",
    "                              [i/MAX_SHARE for i in self.state[FEATURES*STOCK_DIM+1:]]\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "        \n",
    "        return self.state_sc, self.reward, self.terminal, {}\n",
    "        \n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 1\n",
    "        self.terminal = False \n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        return self.state_sc\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c871a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:53.402613Z",
     "iopub.status.busy": "2022-05-16T08:55:53.384238Z",
     "iopub.status.idle": "2022-05-16T08:55:53.405782Z",
     "shell.execute_reply": "2022-05-16T08:55:53.405327Z",
     "shell.execute_reply.started": "2022-05-14T01:39:38.981275Z"
    },
    "papermill": {
     "duration": 0.067424,
     "end_time": "2022-05-16T08:55:53.405895",
     "exception": false,
     "start_time": "2022-05-16T08:55:53.338471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env train 2\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# shares normalization factor\n",
    "# max shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "MAX_ACCOUNT_BALANCE = 100e6\n",
    "MAX_SHARE = 1e6\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "TRANSACTION_FEE_PERCENT = 0\n",
    "REWARD_SCALING = 1e-4\n",
    "# price history\n",
    "FEATURES = len(select_feat)\n",
    "\n",
    "class StockEnvTrain2(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df_sc):\n",
    "\n",
    "        self.day = 1\n",
    "        self.df_sc = df_sc\n",
    "        self.df = sc_X.inverse_transform(df_sc)\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = [Current Balance]+[4 features for 3 stocks]+[owned shares for 3 stocks] \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (1+FEATURES*STOCK_DIM+STOCK_DIM,))\n",
    "        self.terminal = False     \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        \n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self._seed()\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        # update balance\n",
    "        # sell the amount suggested by action, but not more than the amount in the port\n",
    "        self.state[0] += \\\n",
    "        self.state[index*FEATURES+1]*min(abs(action),self.state[index+STOCK_DIM*FEATURES+1]) * \\\n",
    "         (1- TRANSACTION_FEE_PERCENT)\n",
    "        # update amount in the port\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] -= min(abs(action), self.state[index+STOCK_DIM*FEATURES+1])\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index*FEATURES+1]\n",
    "        # update cash. buy the amount suggested by action, limited by available cash\n",
    "        self.state[0] -= self.state[index*FEATURES+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)        \n",
    "        # update stocks balance\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] += min(available_amount, action)\n",
    "    \n",
    "    def step(self, actions):\n",
    "\n",
    "        self.terminal = self.day == self.df.shape[0]-1\n",
    "\n",
    "        if self.terminal:\n",
    "            return self.state_sc, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # normalize \n",
    "            actions = actions * HMAX_NORMALIZE   \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]           \n",
    "            \n",
    "            for index in sell_index:\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "\n",
    "            #load next state\n",
    "            self.state = [self.state[0]] +\\\n",
    "                          self.df[self.day-1].tolist() +\\\n",
    "                          self.state[FEATURES*STOCK_DIM+1:]\n",
    "        \n",
    "            self.state_sc = [self.state[0]/MAX_ACCOUNT_BALANCE] +\\\n",
    "                              self.df_sc[self.day-1].tolist() +\\\n",
    "                              [i/MAX_SHARE for i in self.state[FEATURES*STOCK_DIM+1:]]\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "        \n",
    "        return self.state_sc, self.reward, self.terminal, {}\n",
    "        \n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 1\n",
    "        self.terminal = False \n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        return self.state_sc\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1d9e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:53.506541Z",
     "iopub.status.busy": "2022-05-16T08:55:53.488219Z",
     "iopub.status.idle": "2022-05-16T08:55:53.509264Z",
     "shell.execute_reply": "2022-05-16T08:55:53.508837Z",
     "shell.execute_reply.started": "2022-05-14T01:39:40.366725Z"
    },
    "papermill": {
     "duration": 0.066649,
     "end_time": "2022-05-16T08:55:53.509385",
     "exception": false,
     "start_time": "2022-05-16T08:55:53.442736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# env val\n",
    "\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# shares normalization factor\n",
    "# max shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "MAX_ACCOUNT_BALANCE = 100e6\n",
    "MAX_SHARE = 1e6\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 3\n",
    "TRANSACTION_FEE_PERCENT = 0\n",
    "REWARD_SCALING = 1e-4\n",
    "# price history\n",
    "FEATURES = len(select_feat)\n",
    "\n",
    "class StockEnvVal(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df_sc):\n",
    "\n",
    "        self.day = length\n",
    "        self.df_sc = df_sc\n",
    "        self.df = sc_X.inverse_transform(df_sc)\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = [Current Balance]+[4 features for 3 stocks]+[owned shares for 3 stocks] \n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (1+FEATURES*STOCK_DIM+STOCK_DIM,))\n",
    "        self.terminal = False     \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        \n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self._seed()\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        # update balance\n",
    "        # sell the amount suggested by action, but not more than the amount in the port\n",
    "        self.state[0] += \\\n",
    "        self.state[index*FEATURES+1]*min(abs(action),self.state[index+STOCK_DIM*FEATURES+1]) * \\\n",
    "         (1- TRANSACTION_FEE_PERCENT)\n",
    "        # update amount in the port\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] -= min(abs(action), self.state[index+STOCK_DIM*FEATURES+1])\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index*FEATURES+1]\n",
    "        # update cash. buy the amount suggested by action, limited by available cash\n",
    "        self.state[0] -= self.state[index*FEATURES+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)        \n",
    "        # update stocks balance\n",
    "        self.state[index+STOCK_DIM*FEATURES+1] += min(available_amount, action)\n",
    "    \n",
    "    def step(self, actions):\n",
    "\n",
    "        self.terminal = self.day == self.df.shape[0]-1\n",
    "\n",
    "        if self.terminal:\n",
    "            return self.state_sc, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # normalize \n",
    "            actions = actions * HMAX_NORMALIZE   \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]           \n",
    "            \n",
    "            for index in sell_index:\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "\n",
    "            #load next state\n",
    "            self.state = [self.state[0]] +\\\n",
    "                          self.df[self.day-1].tolist() +\\\n",
    "                          self.state[FEATURES*STOCK_DIM+1:]\n",
    "        \n",
    "            self.state_sc = [self.state[0]/MAX_ACCOUNT_BALANCE] +\\\n",
    "                              self.df_sc[self.day-1].tolist() +\\\n",
    "                              [i/MAX_SHARE for i in self.state[FEATURES*STOCK_DIM+1:]]\n",
    "\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "                sum( \n",
    "                np.array([self.state[i*FEATURES+1] for i in range(STOCK_DIM)]) *\\\n",
    "                np.array(self.state[FEATURES*STOCK_DIM+1:])\n",
    "                )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "        \n",
    "        return self.state_sc, self.reward, self.terminal, {}\n",
    "        \n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = length\n",
    "        self.terminal = False \n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] +\\\n",
    "                      self.df[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "        self.state_sc = [INITIAL_ACCOUNT_BALANCE/MAX_ACCOUNT_BALANCE] +\\\n",
    "                      self.df_sc[self.day-1].tolist() +\\\n",
    "                      [0]*STOCK_DIM\n",
    "\n",
    "        return self.state_sc\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c13037",
   "metadata": {
    "papermill": {
     "duration": 0.036477,
     "end_time": "2022-05-16T08:55:53.582320",
     "exception": false,
     "start_time": "2022-05-16T08:55:53.545843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* A good practice is to use multiple environments in parallel. But due to some errors, I could not. So I only used 1 environment for training.\n",
    "* A random hyperparameter tuning is used here. But again, to not repeat the search work, I only show the best parameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c161e577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T08:55:53.676437Z",
     "iopub.status.busy": "2022-05-16T08:55:53.675592Z",
     "iopub.status.idle": "2022-05-16T09:51:32.830493Z",
     "shell.execute_reply": "2022-05-16T09:51:32.831651Z",
     "shell.execute_reply.started": "2022-05-14T01:39:42.067927Z"
    },
    "papermill": {
     "duration": 3339.213789,
     "end_time": "2022-05-16T09:51:32.831959",
     "exception": false,
     "start_time": "2022-05-16T08:55:53.618170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================training: {'lr': 0.001, 'n_steps': 256, 'batch_size': 128, 'ent_coef': 1e-06}==========================\n",
      "Eval num_timesteps=3800, episode_reward=677.31 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3800, episode_reward=20.91 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7600, episode_reward=1486.01 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7600, episode_reward=47.85 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11400, episode_reward=2459.32 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11400, episode_reward=55.84 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15200, episode_reward=3120.40 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15200, episode_reward=62.68 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=4577.46 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=71.40 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22800, episode_reward=4487.56 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=22800, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26600, episode_reward=4644.45 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26600, episode_reward=71.35 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=30400, episode_reward=4513.33 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=30400, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=34200, episode_reward=4775.03 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34200, episode_reward=71.14 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=4490.83 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=41800, episode_reward=4493.75 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=41800, episode_reward=71.34 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=45600, episode_reward=4503.75 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=45600, episode_reward=71.34 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=49400, episode_reward=4503.37 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=49400, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=53200, episode_reward=4495.02 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=53200, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=4494.32 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=71.29 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=60800, episode_reward=4500.41 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=60800, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=64600, episode_reward=4505.76 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=64600, episode_reward=71.39 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=68400, episode_reward=4504.25 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=68400, episode_reward=71.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=72200, episode_reward=4502.86 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=72200, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=4501.68 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=71.29 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=79800, episode_reward=4496.11 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=79800, episode_reward=71.18 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=83600, episode_reward=4496.79 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=83600, episode_reward=71.22 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=87400, episode_reward=4504.09 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=87400, episode_reward=71.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=91200, episode_reward=4506.96 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=91200, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=4508.47 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=98800, episode_reward=4504.21 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=98800, episode_reward=71.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=102600, episode_reward=4501.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=102600, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=106400, episode_reward=4495.20 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=106400, episode_reward=71.23 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=110200, episode_reward=4495.32 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=110200, episode_reward=71.23 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=4495.49 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=114000, episode_reward=71.25 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=117800, episode_reward=4503.88 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=117800, episode_reward=71.20 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=121600, episode_reward=4574.39 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=121600, episode_reward=71.28 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=125400, episode_reward=4548.63 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=125400, episode_reward=71.26 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=129200, episode_reward=4501.31 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=129200, episode_reward=71.31 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=4496.59 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=133000, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=136800, episode_reward=4500.52 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=136800, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=140600, episode_reward=4488.07 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=140600, episode_reward=71.28 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=144400, episode_reward=4487.60 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=144400, episode_reward=71.20 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=148200, episode_reward=4500.10 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=148200, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=4495.66 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=155800, episode_reward=4495.62 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=155800, episode_reward=71.22 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=159600, episode_reward=4492.94 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=159600, episode_reward=71.25 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=163400, episode_reward=4493.03 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=163400, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=167200, episode_reward=4742.35 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=167200, episode_reward=71.29 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=4522.58 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=171000, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=174800, episode_reward=4562.22 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=174800, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=178600, episode_reward=4644.37 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=178600, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=182400, episode_reward=4724.84 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=182400, episode_reward=71.17 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=186200, episode_reward=4727.16 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=186200, episode_reward=71.24 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=2493.04 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=193800, episode_reward=4496.58 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=193800, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=197600, episode_reward=4527.43 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=197600, episode_reward=71.28 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=201400, episode_reward=4583.26 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=201400, episode_reward=71.23 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=205200, episode_reward=4495.74 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=205200, episode_reward=71.17 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=4504.09 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=209000, episode_reward=70.91 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=212800, episode_reward=4500.42 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=212800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=216600, episode_reward=4540.83 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=216600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=220400, episode_reward=4575.24 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=220400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=224200, episode_reward=4495.30 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=224200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=4495.82 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=231800, episode_reward=4495.91 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=231800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=235600, episode_reward=4504.38 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=235600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=239400, episode_reward=4497.05 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=239400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=243200, episode_reward=4498.19 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=243200, episode_reward=71.09 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=4499.15 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=247000, episode_reward=71.17 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=250800, episode_reward=4488.77 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=250800, episode_reward=71.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=254600, episode_reward=4503.18 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=254600, episode_reward=71.40 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=258400, episode_reward=4496.93 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=258400, episode_reward=85.42 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=262200, episode_reward=4496.12 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=262200, episode_reward=71.81 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=4498.49 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=266000, episode_reward=71.34 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=269800, episode_reward=4506.76 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=269800, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=273600, episode_reward=4506.64 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=273600, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=277400, episode_reward=4505.31 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=277400, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=281200, episode_reward=4504.75 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=281200, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=4497.30 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=288800, episode_reward=4496.20 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=288800, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=292600, episode_reward=4497.04 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=292600, episode_reward=71.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=296400, episode_reward=4496.20 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=296400, episode_reward=71.28 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=300200, episode_reward=4497.60 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=300200, episode_reward=71.19 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=4501.41 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=71.11 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=307800, episode_reward=4498.55 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=307800, episode_reward=71.12 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=311600, episode_reward=4497.97 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=311600, episode_reward=71.20 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=315400, episode_reward=4501.74 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=315400, episode_reward=71.31 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=319200, episode_reward=4508.26 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=319200, episode_reward=71.32 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=4514.40 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=323000, episode_reward=71.27 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=326800, episode_reward=4498.41 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=326800, episode_reward=71.29 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=330600, episode_reward=4508.02 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=330600, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=334400, episode_reward=4507.03 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=334400, episode_reward=71.40 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=338200, episode_reward=4501.07 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=338200, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=4498.07 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=342000, episode_reward=71.27 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=345800, episode_reward=4495.94 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=345800, episode_reward=71.24 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=349600, episode_reward=4502.92 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=349600, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=353400, episode_reward=4505.29 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=353400, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=357200, episode_reward=4513.80 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=357200, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=4513.27 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=361000, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=364800, episode_reward=4503.61 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=364800, episode_reward=71.30 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=368600, episode_reward=4499.79 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=368600, episode_reward=71.23 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=372400, episode_reward=4506.81 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=372400, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=376200, episode_reward=4511.10 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=376200, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=4506.40 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=71.34 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=383800, episode_reward=4505.22 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=383800, episode_reward=71.35 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=387600, episode_reward=4514.55 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=387600, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=391400, episode_reward=4512.66 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=391400, episode_reward=71.44 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=395200, episode_reward=4500.02 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=395200, episode_reward=71.33 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=4494.79 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=399000, episode_reward=71.37 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=402800, episode_reward=4492.69 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=402800, episode_reward=71.25 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=406600, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=406600, episode_reward=71.09 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=410400, episode_reward=4501.04 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=410400, episode_reward=71.24 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=414200, episode_reward=4583.52 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=414200, episode_reward=71.09 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=4684.23 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=418000, episode_reward=71.18 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=421800, episode_reward=4826.37 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=421800, episode_reward=71.44 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=425600, episode_reward=4821.58 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=425600, episode_reward=71.53 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=429400, episode_reward=4841.30 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=429400, episode_reward=71.50 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=433200, episode_reward=4583.79 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=433200, episode_reward=70.94 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=4667.79 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=437000, episode_reward=70.89 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=440800, episode_reward=4647.17 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=440800, episode_reward=71.00 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=444600, episode_reward=4554.94 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=444600, episode_reward=70.90 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=448400, episode_reward=4613.82 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=448400, episode_reward=70.88 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=452200, episode_reward=4576.65 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=452200, episode_reward=70.86 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=4603.44 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=71.01 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=459800, episode_reward=4588.45 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=459800, episode_reward=70.88 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=463600, episode_reward=4651.55 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=463600, episode_reward=70.84 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=467400, episode_reward=4500.12 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=467400, episode_reward=70.99 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=471200, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=471200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=4497.71 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=478800, episode_reward=4497.80 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=478800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=482600, episode_reward=4497.71 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=482600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=486400, episode_reward=4497.60 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=486400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=490200, episode_reward=4497.70 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=490200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=494000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=497800, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=497800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=501600, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=501600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=505400, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=505400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=509200, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=509200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=513000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=516800, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=516800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=520600, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=520600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=524400, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=524400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=528200, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=528200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=535800, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=535800, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=539600, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=539600, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=543400, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=543400, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=547200, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=547200, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=4497.57 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=551000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=554800, episode_reward=4578.53 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=554800, episode_reward=71.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=558600, episode_reward=5631.22 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=558600, episode_reward=71.17 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=562400, episode_reward=3715.66 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=562400, episode_reward=70.05 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=566200, episode_reward=2332.31 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=566200, episode_reward=61.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=3742.47 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=69.43 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=573800, episode_reward=3277.09 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=573800, episode_reward=70.17 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=577600, episode_reward=5165.78 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=577600, episode_reward=71.22 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=581400, episode_reward=5373.69 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=581400, episode_reward=70.86 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=585200, episode_reward=6021.53 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=585200, episode_reward=70.68 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=5620.18 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=589000, episode_reward=70.77 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=592800, episode_reward=3600.13 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=592800, episode_reward=69.98 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=596600, episode_reward=3420.26 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=596600, episode_reward=69.66 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=600400, episode_reward=3921.97 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=600400, episode_reward=66.82 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=604200, episode_reward=2372.78 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=604200, episode_reward=65.09 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=2203.54 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=62.29 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=611800, episode_reward=2755.34 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=611800, episode_reward=67.22 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=615600, episode_reward=2137.98 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=615600, episode_reward=62.09 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=619400, episode_reward=2040.54 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=619400, episode_reward=62.27 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=623200, episode_reward=1780.08 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=623200, episode_reward=21.06 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=1921.47 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=627000, episode_reward=7.82 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=630800, episode_reward=2215.94 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=630800, episode_reward=9.47 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=634600, episode_reward=3785.92 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=634600, episode_reward=14.04 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=638400, episode_reward=3847.18 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=638400, episode_reward=14.90 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=642200, episode_reward=2750.26 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=642200, episode_reward=8.72 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=1909.49 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=646000, episode_reward=2.10 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=649800, episode_reward=2293.92 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=649800, episode_reward=10.62 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=653600, episode_reward=1536.67 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=653600, episode_reward=27.38 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=657400, episode_reward=1672.91 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=657400, episode_reward=24.06 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=661200, episode_reward=1088.32 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=661200, episode_reward=27.74 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1088.48 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=27.64 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=668800, episode_reward=1102.60 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=668800, episode_reward=27.73 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=672600, episode_reward=1710.49 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=672600, episode_reward=38.87 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=676400, episode_reward=1746.19 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=676400, episode_reward=42.66 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=680200, episode_reward=1898.44 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=680200, episode_reward=43.84 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=1815.00 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=40.61 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=687800, episode_reward=1840.77 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=687800, episode_reward=47.48 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=691600, episode_reward=2239.83 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=691600, episode_reward=55.82 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=695400, episode_reward=3173.19 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=695400, episode_reward=69.66 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=699200, episode_reward=2747.16 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=699200, episode_reward=69.16 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=3047.02 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=703000, episode_reward=68.99 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=706800, episode_reward=2275.46 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=706800, episode_reward=56.78 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=710600, episode_reward=2310.68 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=710600, episode_reward=61.55 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=714400, episode_reward=1769.78 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=714400, episode_reward=45.14 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=718200, episode_reward=1881.86 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=718200, episode_reward=46.78 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=2001.27 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=722000, episode_reward=50.63 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=725800, episode_reward=3396.61 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=725800, episode_reward=70.96 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=729600, episode_reward=2226.39 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=729600, episode_reward=68.06 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=733400, episode_reward=2406.81 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=733400, episode_reward=69.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=737200, episode_reward=3024.47 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=737200, episode_reward=70.81 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=2495.56 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=741000, episode_reward=56.02 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=744800, episode_reward=3082.38 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=744800, episode_reward=70.36 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=748600, episode_reward=2551.76 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=748600, episode_reward=60.37 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=752400, episode_reward=2457.16 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=752400, episode_reward=60.05 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=756200, episode_reward=3040.55 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=756200, episode_reward=58.87 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=3874.72 +/- 0.00\n",
      "Episode length: 3746.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=71.59 +/- 0.00\n",
      "Episode length: 254.00 +/- 0.00\n",
      "version:  0 Training time (PPO):  55.58635015885035  minutes\n",
      "train performance: 45969295.19768979, val performance: 1854176.5235364437\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import time\n",
    "from numpy.random import choice\n",
    "\n",
    "# clearing gpu cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# adding extra day \n",
    "df_train_rl = np.append(X_train, X_train[-1].reshape(1,-1), axis=0)\n",
    "df_val_rl = np.append(X_val, X_val[-1].reshape(1,-1), axis=0)\n",
    "\n",
    "\n",
    "# building environment \n",
    "# single environment\n",
    "n_env = 1\n",
    "env_train = DummyVecEnv([lambda: StockEnvTrain(df_train_rl)])\n",
    "env_train2 = DummyVecEnv([lambda: StockEnvTrain2(df_train_rl)])\n",
    "env_val = DummyVecEnv([lambda: StockEnvVal(df_val_rl)])\n",
    "\n",
    "# env_train.reset()\n",
    "# env_train2.reset()\n",
    "# env_val.reset()\n",
    "\n",
    "# model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "import math\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# random search params\n",
    "def build_config(samples):\n",
    "    \n",
    "    lr = [1e-3, 1e-4, 1e-5]\n",
    "    n_steps = [256,512,1024,2048]\n",
    "    batch_size = [128,256,512,1024]\n",
    "    ent_coef = [1e-6,1e-7,1e-8]\n",
    "    \n",
    "    config = []\n",
    "    i = 0\n",
    "    lim = 100\n",
    "                  \n",
    "    while len(config) < samples:\n",
    "            \n",
    "        params = {}\n",
    "        params['lr'] = choice(lr)\n",
    "        params['n_steps'] = choice(n_steps)\n",
    "        params['batch_size'] = choice(batch_size)\n",
    "        params['ent_coef'] = choice(ent_coef)\n",
    "\n",
    "        if params not in config:\n",
    "            config.append(params)\n",
    "            \n",
    "        i += 1\n",
    "        if i == lim: sys.exit('error')\n",
    "        \n",
    "    return config\n",
    "\n",
    "def perf_plot(version):\n",
    "    \n",
    "    data = np.load(f\"./logs/m{version}_train/evaluations.npz\")\n",
    "    train_result = [10**6 + total_r*1/REWARD_SCALING for total_r in np.mean(data['results'], axis=1)]\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('training performance')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('asset')\n",
    "    plt.plot(np.arange(len(train_result)), train_result)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # validation\n",
    "    data = np.load(f\"./logs/m{version}_val/evaluations.npz\")\n",
    "    val_result = [10**6 + total_r*1/REWARD_SCALING for total_r in np.mean(data['results'], axis=1)]\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('val performance')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('asset')\n",
    "    plt.plot(np.arange(len(val_result)), val_result)\n",
    "\n",
    "    best_val_id = np.argmax(val_result)\n",
    "    print('train performance: {}, val performance: {}'.format(train_result[best_val_id], val_result[best_val_id]))\n",
    "\n",
    "def train_PPO(env_train, env_train2, env_val, version, params, timesteps):\n",
    "    start = time.time()\n",
    "    eval_train = EvalCallback(env_train2, \n",
    "                             log_path=f\"./logs/m{version}_train/\", \n",
    "                             n_eval_episodes = 3,\n",
    "                             eval_freq=3800//n_env,\n",
    "                             deterministic=True, \n",
    "                             render=False)\n",
    "\n",
    "    eval_val = EvalCallback(env_val, \n",
    "                             log_path=f\"./logs/m{version}_val/\", \n",
    "                             best_model_save_path =f\"./mod/m{version}_val/\",\n",
    "                             n_eval_episodes = 3,\n",
    "                             eval_freq=3800//n_env,\n",
    "#                              eval_freq=(params['n_steps']+10)//n_env,\n",
    "                             deterministic=True, \n",
    "                             render=False)\n",
    "\n",
    "    model = PPO('MlpPolicy', \n",
    "                env=env_train,\n",
    "                n_steps = params['n_steps'],\n",
    "                learning_rate = params['lr'],\n",
    "                ent_coef =  params['ent_coef'],\n",
    "                batch_size = params['batch_size']\n",
    "                )\n",
    "\n",
    "    model.learn(total_timesteps=timesteps,\n",
    "                callback=[eval_train, eval_val]\n",
    "               )\n",
    "    end = time.time()\n",
    "    print('version: ', version, 'Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    perf_plot(version)\n",
    "    \n",
    "# start training\n",
    "config = build_config(5)\n",
    "# for version, params in enumerate(config):\n",
    "#     print(f'========================training: {params}==========================')\n",
    "#     train_PPO(env_train, env_train2, env_val, version, params, timesteps=760000)\n",
    "params = {'lr': 0.001, 'n_steps': 256, 'batch_size': 128, 'ent_coef': 1e-06}\n",
    "\n",
    "\n",
    "version = 0\n",
    "print(f'========================training: {params}==========================')\n",
    "train_PPO(env_train, env_train2, env_val, version, params, timesteps=760000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18b7f0",
   "metadata": {
    "papermill": {
     "duration": 0.139562,
     "end_time": "2022-05-16T09:51:33.137885",
     "exception": false,
     "start_time": "2022-05-16T09:51:32.998323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* The result is just a little bit lower than the previous strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693b190",
   "metadata": {
    "papermill": {
     "duration": 0.141436,
     "end_time": "2022-05-16T09:51:33.417518",
     "exception": false,
     "start_time": "2022-05-16T09:51:33.276082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "reference\n",
    "\n",
    "@article{finrl2020,\n",
    "    author  = {Liu, Xiao-Yang and Yang, Hongyang and Chen, Qian and Zhang, Runjia and Yang, Liuqing and Xiao, Bowen and Wang, Christina Dan},\n",
    "    title   = {{FinRL}: A deep reinforcement learning library for automated stock trading in quantitative finance},\n",
    "    journal = {Deep RL Workshop, NeurIPS 2020},\n",
    "    year    = {2020}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3441.780309,
   "end_time": "2022-05-16T09:51:34.672513",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-16T08:54:12.892204",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
